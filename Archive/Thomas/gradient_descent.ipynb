{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5750e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c53ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "\n",
    "f = lambda x: 1/(1+25*x**2)\n",
    "np.random.seed(123)\n",
    "n_samples = 1000\n",
    "x_=np.random.uniform(-1,1,n_samples)\n",
    "noise = np.random.normal(loc=0, scale=0.1, size=n_samples)\n",
    "y_with_noise = f(x_) + noise\n",
    "degree = 5\n",
    "X = PolynomialFeatures(degree).fit_transform(x_.reshape(-1,1))\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "y_centered = y_with_noise - y_with_noise.mean()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_centered, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0afb2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define makers for the objectives of OLS, Ridge and Lasso\n",
    "#It returns the corresponding objective: a one dimensional function\n",
    "#which only depends on theta and is minimized in regression\n",
    "def maker_objective_ols(X,y):\n",
    "    def objective_ols(theta):\n",
    "        return mean_squared_error(X @ theta, y)\n",
    "    return objective_ols\n",
    "\n",
    "\n",
    "def maker_objective_ridge(X, y, lmbda):\n",
    "    def objective_ridge(theta):\n",
    "        return mean_squared_error(X @ theta, y) + lmbda * np.sum(theta**2)\n",
    "    return objective_ridge\n",
    "\n",
    "def maker_objective_lasso(X, y, lmbda):\n",
    "    def objective_lasso(theta):\n",
    "        return mean_squared_error(X @ theta, y) + lmbda *np.sum(np.abs(theta))\n",
    "    return objective_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5c940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradients for OLS, Ridge and Lasso\n",
    "#For Ridge and Lasso a maker is used which takes lambda as input and returns\n",
    "#the corresponding gradient\n",
    "#definitions a\n",
    "#Parameters of the gradients:\n",
    "#xtx: np.array matrix product of X.T @ X\n",
    "#xty: np.array matrix product of X.T @ y\n",
    "#theta: np.array \n",
    "#n_samples: int number of samples i.e. the number of rows of X\n",
    "\n",
    "def gradient_ols(xtx, xty, theta, n_samples):\n",
    "    return 2/n_samples* (xtx @theta - xty)\n",
    "\n",
    "def maker_gradient_ridge(lmbda):\n",
    "    def gradient_ridge(xtx, xty, theta, n_samples):\n",
    "        return (xtx @theta - xty) + lmbda*theta\n",
    "    return gradient_ridge\n",
    "\n",
    "def maker_gradient_lasso(lmbda):\n",
    "    def gradient_lasso(xtx, xty, theta, n_samples):\n",
    "        return 2/n_samples*(xtx @theta - xty) + lmbda*np.sign(theta)\n",
    "    return gradient_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9720edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the effective gradient, i.e. for non stochastic gradient the normal\n",
    "#gradient with precalculated xtx and xty and for stochastic gradient random batches are created,\n",
    "#the gradient for every batch is calculated and effective gradient is the average gradient of all batches\n",
    "#is used in the gradient descent implementations\n",
    "#parameters:\n",
    "#X np.array\n",
    "#y np.array\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch_size: int used for gradient descent\n",
    "def gradient_eff_calculator(X, y, gradient, stochastic, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    if stochastic:\n",
    "        def gradient_eff(theta):\n",
    "            shuffled_indices = np.random.choice(range(n_samples), n_samples, replace = False)\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            m = int(n_samples/batch_size) # number of batches\n",
    "            array_batch_gradients = np.zeros((m, n_features))\n",
    "            for i in range(m):\n",
    "                xi = X_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "                yi = y_shuffled[i*batch_size: (i+1)*batch_size] #exclude the last samples?\n",
    "                batch_gradient_i = gradient(xi.T @ xi, xi.T @ yi, theta, n_samples)\n",
    "                array_batch_gradients[i] = batch_gradient_i\n",
    "            return np.mean(array_batch_gradients, axis = 0)\n",
    "    else:\n",
    "        xtx = X.T @ X\n",
    "        xty = X.T @ y\n",
    "        def gradient_eff(theta):\n",
    "            return gradient(xtx, xty, theta, n_samples)    \n",
    "    return gradient_eff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0663beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a random initial value for the gradient decents methods\n",
    "#multivariate uniform distribution where the bounds for every feature\n",
    "#are the corresponding minima and maxima in X\n",
    "#used in the gradient descents methods\n",
    "def initial_value_generator(X):\n",
    "    n_features = X.shape[1]\n",
    "    return np.min(X, axis =0) + np.random.uniform(size =n_features) * (np.max(X, axis =0) - np.min(X, axis =0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ebf3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a normal gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_normal(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter:\n",
    "        theta_old = theta_new\n",
    "        theta_new = theta_old - learning_rate * gradient_eff(theta_old)\n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d37525c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.00282054, -0.66959885, -0.00840943,  0.47905071,\n",
       "         0.00443768]),\n",
       " 6441]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_normal(X_train, y_train, gradient_ols, learning_rate= 0.1, max_iter= 100000, precision=0.000001, stochastic=False, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79ef59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a momentum gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#momentum: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_momentum(X, y, gradient, learning_rate, momentum, max_iter, precision, stochastic, batch_size, initial_value = None):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    change = 0\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        theta_old = theta_new\n",
    "        change = learning_rate * gradient_eff(theta_old) + momentum * change\n",
    "        theta_new = theta_old - change \n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4aba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a Adagrad gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#epsilon: float used for gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_adagrad(X, y, gradient, learning_rate, epsilon, max_iter, precision, stochastic, batch_size, initial_value = None):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    G = np.zeros(n_features)\n",
    "    gradient1 = np.zeros(n_features)\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter:\n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        G = G + np.square(gradient1)\n",
    "        theta_new = theta_old - learning_rate * gradient1 / np.sqrt(epsilon + G)\n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81bc2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a RMS Prop gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#epsilon: float used for gradient descent\n",
    "#rho: float used for gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_rmsprop(X, y, gradient, learning_rate, epsilon, rho, max_iter, precision, stochastic, batch_size, initial_value = None):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "        \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + np.ones(n_features)\n",
    "    v = np.zeros(n_features)\n",
    "    gradient1 = np.zeros(n_features)\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        v= rho * v + (1-rho) * gradient1**2\n",
    "        theta_new = theta_old - learning_rate / np.sqrt(v + epsilon) * gradient1\n",
    "        count += 1    \n",
    "    \n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "794b20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a Adam gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#epsilon: float used for gradient descent\n",
    "#beta_1: float used for gradient descent\n",
    "#beta_2: float used for gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_adam(X, y, gradient, learning_rate, epsilon, beta_1, beta_2, max_iter, precision, stochastic, batch_size, initial_value = None):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + np.ones(n_features)\n",
    "    count = 0\n",
    "    m = 0\n",
    "    v = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        count+=1\n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        m = beta_1 * m + (1-beta_1)*gradient1\n",
    "        v = beta_2 * v + (1-beta_2)*gradient1**2\n",
    "        m_tilde = m/(1-beta_1**count)\n",
    "        v_tilde = v/(1-beta_2**count)\n",
    "        theta_new = theta_old - learning_rate * m_tilde / (np.sqrt(v_tilde) + epsilon)\n",
    "\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbc31f",
   "metadata": {},
   "source": [
    "Finding the optimal learning rates via Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f19351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "# tuning to find the best hyperparameters\n",
    "# parameters:\n",
    "# function: optimizer function which depends on one parameter and returns a list with the calculated optimium\n",
    "# and the number of iterations, particularly the gradient descent methods are such functions if all input parameters are fixed\n",
    "# apart from one hyperparameter\n",
    "# parameters: list of the parameters which are possible input values for function\n",
    "# objective: objective of the optimizer function, possible functions are\n",
    "# objective_ols or the returned values of maker_gradient_ridge or maker_gradient_lasso\n",
    "# returns:\n",
    "# a list containing a Pandas DataFrame with the columns Parameters, Result Objective and Number of iterations and dictionary with the optimum for each parameter\n",
    "def tuning(function, parameters, objective):\n",
    "    result_objective = np.zeros(len(parameters))\n",
    "    result_iterations = np.zeros(len(parameters))\n",
    "    result_optimum = {}\n",
    "    for i, parameter in enumerate(parameters):\n",
    "        print(f\"test parameter {parameter}\")\n",
    "        try:\n",
    "            function_run = function(parameter)\n",
    "            result_objective[i] = objective(function_run[0])\n",
    "            result_iterations[i] = function_run[1]\n",
    "            result_optimum[parameter] = function_run[0]\n",
    "        except RuntimeWarning:\n",
    "            result_objective[i] = np.inf\n",
    "            print(\"error\")\n",
    "\n",
    "    dataframe = pd.DataFrame({\"Parameters\": parameters, \"Result Objective\":result_objective, \"Number of Iterations\": result_iterations})\n",
    "    return [dataframe, result_optimum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ea0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          0.894485               10000.0\n",
      "1    0.000010          1.751385               10000.0\n",
      "2    0.000100          0.249079               10000.0\n",
      "3    0.001000          0.050915               10000.0\n",
      "4    0.010000          0.027417               10000.0\n",
      "5    0.100000          0.025538                7052.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent ols non stochastic\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, gradient_ols, learning_rate =param, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_normal_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3053db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00337317 -0.66959392 -0.00999685  0.47904601  0.00554336]\n"
     ]
    }
   ],
   "source": [
    "values_normal_ols_nonstochastic = results_normal_ols_nonstochastic[1][0.1]\n",
    "print(values_normal_ols_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bd2bb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "error\n",
      "test parameter 0.01\n",
      "error\n",
      "test parameter 0.1\n",
      "error\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          0.677197               10000.0\n",
      "1    0.000010          0.106873               10000.0\n",
      "2    0.000100          0.092915               10000.0\n",
      "3    0.001000               inf                   0.0\n",
      "4    0.010000               inf                   0.0\n",
      "5    0.100000               inf                   0.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent ridge non stochastic\n",
    "lmbda = 0.1\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, maker_gradient_ridge(lmbda), learning_rate =param, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_ridge_nonstochastic = tuning(function, parameters, maker_objective_ridge(X_train, y_train, lmbda = lmbda))\n",
    "print(results_normal_ridge_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d543daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.0071101  -0.66751743 -0.02075463  0.47698795  0.01303614]\n"
     ]
    }
   ],
   "source": [
    "values_normal_ridge_nonstochastic = results_normal_ridge_nonstochastic[1][0.0001]\n",
    "print(values_normal_ridge_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf3fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.00302554, -0.6676647 , -0.00904645,  0.47710896,\n",
       "        0.00488546])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ridge(alpha =0.1).fit(X_train, y_train).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f598c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.00301674, -0.66755393, -0.00899664,  0.47702275,\n",
       "         0.00484636]),\n",
       " 28860]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_normal(X_train, y_train, maker_gradient_ridge(lmbda), learning_rate =0.0001, max_iter = 50000, precision = 0.00000001, stochastic = False, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          5.595031               50000.0\n",
      "1    0.000010          1.297930               50000.0\n",
      "2    0.000100          0.194103               50000.0\n",
      "3    0.001000          0.062086               50000.0\n",
      "4    0.010000          0.062274               50000.0\n",
      "5    0.100000          0.063995               50000.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent lasso non stochastic\n",
    "lmbda = 0.1\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, maker_gradient_lasso(lmbda), learning_rate =param, max_iter = 50000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_lasso_nonstochastic = tuning(function, parameters, maker_objective_lasso(X_train, y_train, lmbda = lmbda))\n",
    "print(results_normal_lasso_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf55989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  2.42827063e-05 -1.59179744e-01 -5.15196932e-05\n",
      " -7.37363208e-05 -2.86472326e-05]\n"
     ]
    }
   ],
   "source": [
    "values_normal_lasso_nonstochastic = results_normal_lasso_nonstochastic[1][0.001]\n",
    "print(values_normal_lasso_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e896c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "calculation limit exceeded\n",
      "test parameter 10\n",
      "calculation limit exceeded\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          5.595031               50000.0\n",
      "1    0.000010          1.297930               50000.0\n",
      "2    0.000100          0.194103               50000.0\n",
      "3    0.001000          0.062086               50000.0\n",
      "4    0.010000          0.062274               50000.0\n",
      "5    0.100000          0.063995               50000.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for adam gradient descent lasso non stochastic\n",
    "lmbda = 0.1\n",
    "function = lambda param: gradient_descent_adam(X_train, y_train, maker_gradient_lasso(lmbda), learning_rate =param, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-7,max_iter = 50000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_adam_lasso_nonstochastic = tuning(function, parameters, maker_objective_lasso(X_train, y_train, lmbda = lmbda))\n",
    "print(results_normal_lasso_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57900fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  2.42827063e-05 -1.59179744e-01 -5.15196932e-05\n",
      " -7.37363208e-05 -2.86472326e-05]\n"
     ]
    }
   ],
   "source": [
    "values_normal_lasso_nonstochastic = results_normal_lasso_nonstochastic[1][0.001]\n",
    "print(values_normal_lasso_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b8608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       , -0.       , -0.1086299, -0.       , -0.       ,\n",
       "       -0.       ])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso(alpha = 0.1).fit(X_train, y_train).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001         18.829439               10000.0\n",
      "1    0.000010          7.507020               10000.0\n",
      "2    0.000100          1.769454               10000.0\n",
      "3    0.001000          0.126662               10000.0\n",
      "4    0.010000          0.039833               10000.0\n",
      "5    0.100000          0.027670               10000.0\n",
      "6    1.000000          0.025538                4365.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent ols stochastic\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, gradient_ols, learning_rate =param, max_iter = 10000, precision = 0.000001, stochastic = True, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_ols_stochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_normal_ols_stochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605bb4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00329029 -0.66959466 -0.00975878  0.47904672  0.00537754]\n"
     ]
    }
   ],
   "source": [
    "values_normal_ols_stochastic = results_normal_ols_stochastic[1][1]\n",
    "print(values_normal_ols_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf31ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          4.416559               10000.0\n",
      "1    0.000010          9.543490               10000.0\n",
      "2    0.000100          0.353467               10000.0\n",
      "3    0.001000          0.058400               10000.0\n",
      "4    0.010000          0.026223               10000.0\n",
      "5    0.100000          0.025538                5650.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for momentum gradient descent ols\n",
    "function = lambda param: gradient_descent_momentum(X_train, y_train, gradient_ols, learning_rate =param, momentum = 0.3, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_momentum_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_momentum_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4986da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00328993 -0.66959467 -0.00975774  0.47904672  0.00537681]\n"
     ]
    }
   ],
   "source": [
    "values_momentum_ols_nonstochastic = results_momentum_ols_nonstochastic[1][0.1]\n",
    "print(values_momentum_ols_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36043390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "test parameter 1e-05\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "test parameter 10\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001         20.993486                   5.0\n",
      "1    0.000010          7.863187                 500.0\n",
      "2    0.000100          1.709036               10000.0\n",
      "3    0.001000          1.002045               10000.0\n",
      "4    0.010000          0.801262               10000.0\n",
      "5    0.100000          0.025548               10000.0\n",
      "6    1.000000          0.025538                3512.0\n",
      "7   10.000000          0.025538                4568.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for adagrad gradient descent ols\n",
    "function = lambda param: gradient_descent_adagrad(X_train, y_train, gradient_ols, learning_rate =param, epsilon = 1e-7, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_adagrad_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_adagrad_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ed3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00293787 -0.66959774 -0.00874655  0.47904965  0.00467256]\n"
     ]
    }
   ],
   "source": [
    "values_adagrad_ols_nonstochastic = results_adagrad_ols_nonstochastic[1][10]\n",
    "print(values_adagrad_ols_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50562647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "calculation limit exceeded\n",
      "test parameter 10\n",
      "calculation limit exceeded\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          1.894281               10000.0\n",
      "1    0.000010         21.573893               10000.0\n",
      "2    0.000100          0.391023               10000.0\n",
      "3    0.001000          0.025542               10000.0\n",
      "4    0.010000          0.025864               10000.0\n",
      "5    0.100000          0.058106               10000.0\n",
      "6    1.000000          3.282334               10000.0\n",
      "7   10.000000        325.705101               10000.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for rmsprop gradient descent ols\n",
    "function = lambda param: gradient_descent_rmsprop(X_train, y_train, gradient_ols, learning_rate =param, epsilon = 1e-7, rho= 0.9, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_rmsprop_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_rmsprop_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4953b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00359395 -0.66910239 -0.00870584  0.47954231  0.00548765]\n"
     ]
    }
   ],
   "source": [
    "values_rmsprop_ols_nonstochastic = results_rmsprop_ols_nonstochastic[1][0.001]\n",
    "print(values_rmsprop_ols_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "test parameter 0.1\n",
      "test parameter 1\n",
      "test parameter 10\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001         20.148425               10000.0\n",
      "1    0.000010         19.203959               10000.0\n",
      "2    0.000100          0.396866               10000.0\n",
      "3    0.001000          0.028233               10000.0\n",
      "4    0.010000          0.025538                5059.0\n",
      "5    0.100000          0.025538                1285.0\n",
      "6    1.000000          0.025538                 761.0\n",
      "7   10.000000          0.025538                 516.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for adam gradient descent ols\n",
    "function = lambda param: gradient_descent_adam(X_train, y_train, gradient_ols, learning_rate =param, epsilon = 1e-7, beta_1= 0.9, beta_2= 0.999, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_adam_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_adam_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff01b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00310437 -0.66959632 -0.00922472  0.4790483   0.00500555]\n"
     ]
    }
   ],
   "source": [
    "values_adam_ols_nonstochastic = results_adam_ols_nonstochastic[1][10]\n",
    "print(values_adam_ols_nonstochastic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oslo_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
