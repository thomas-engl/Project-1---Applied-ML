{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5750e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6c53ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data\n",
    "\n",
    "f = lambda x: 1/(1+25*x**2)\n",
    "np.random.seed(123)\n",
    "n_samples = 1000\n",
    "x_=np.random.uniform(-1,1,n_samples)\n",
    "noise = np.random.normal(loc=0, scale=0.1, size=n_samples)\n",
    "y_with_noise = f(x_) + noise\n",
    "degree = 5\n",
    "X = PolynomialFeatures(degree).fit_transform(x_.reshape(-1,1))\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "y_centered = y_with_noise - y_with_noise.mean()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_centered, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0afb2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define makers for the objectives of OLS, Ridge and Lasso\n",
    "#It returns the corresponding objective: a one dimensional function\n",
    "#which only depends on theta and is minimized in regression\n",
    "def maker_objective_ols(X,y):\n",
    "    def objective_ols(theta):\n",
    "        return mean_squared_error(X @ theta, y)\n",
    "    return objective_ols\n",
    "\n",
    "def maker_objective_ridge(X, y, lmbda):\n",
    "    def objective_ridge(theta):\n",
    "        return mean_squared_error(X @ theta, y) + lmbda * np.sum(theta**2)\n",
    "    return objective_ridge\n",
    "\n",
    "def maker_objective_lasso(X, y, lmbda):\n",
    "    def objective_lasso(theta):\n",
    "        return mean_squared_error(X @ theta, y) + lmbda *np.sum(np.abs(theta))\n",
    "    return objective_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f5c940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradients for OLS, Ridge and Lasso\n",
    "#For Ridge and Lasso a maker is used which takes lambda as input and returns\n",
    "#the corresponding gradient\n",
    "#definitions a\n",
    "#Parameters of the gradients:\n",
    "#xtx: np.array matrix product of X.T @ X\n",
    "#xty: np.array matrix product of X.T @ y\n",
    "#theta: np.array \n",
    "#n_samples: int number of samples i.e. the number of rows of X\n",
    "\n",
    "def gradient_ols(xtx, xty, theta, n_samples):\n",
    "    return 2/n_samples* (xtx @theta - xty)\n",
    "\n",
    "def maker_gradient_ridge(lmbda):\n",
    "    def gradient_ridge(xtx, xty, theta, n_samples):\n",
    "        return 2/n_samples*(xtx @theta - xty) + lmbda*theta\n",
    "    return gradient_ridge\n",
    "\n",
    "def maker_gradient_lasso(lmbda):\n",
    "    def gradient_lasso(xtx, xty, theta, n_samples):\n",
    "        return 2/n_samples*(xtx @theta - xty) + lmbda*np.sign(theta)\n",
    "    return gradient_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9720edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the effective gradient, i.e. for non stochastic gradient the normal\n",
    "#gradient with precalculated xtx and xty and for stochastic gradient random batches are created,\n",
    "#the gradient for every batch is calculated and effective gradient is the average gradient of all batches\n",
    "#is used in the gradient descent implementations\n",
    "#parameters:\n",
    "#X np.array\n",
    "#y np.array\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch_size: int used for gradient descent\n",
    "def gradient_eff_calculator(X, y, gradient, stochastic, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    if stochastic:\n",
    "        def gradient_eff(theta):\n",
    "            shuffled_indices = np.random.choice(range(n_samples), n_samples, replace = False)\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            m = int(n_samples/batch_size) # number of batches\n",
    "            array_batch_gradients = np.zeros((m, n_features))\n",
    "            for i in range(m):\n",
    "                xi = X_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "                yi = y_shuffled[i*batch_size: (i+1)*batch_size] #exclude the last samples?\n",
    "                batch_gradient_i = gradient(xi.T @ xi, xi.T @ yi, theta, n_samples)\n",
    "                array_batch_gradients[i] = batch_gradient_i\n",
    "            return np.mean(array_batch_gradients, axis = 0)\n",
    "    else:\n",
    "        xtx = X.T @ X\n",
    "        xty = X.T @ y\n",
    "        def gradient_eff(theta):\n",
    "            return gradient(xtx, xty, theta, n_samples)    \n",
    "    return gradient_eff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0663beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a random initial value for the gradient decents methods\n",
    "#multivariate uniform distribution where the bounds for every feature\n",
    "#are the corresponding minima and maxima in X\n",
    "#used in the gradient descents methods\n",
    "def initial_value_generator(X):\n",
    "    n_features = X.shape[1]\n",
    "    return np.min(X, axis =0) + np.random.uniform(size =n_features) * (np.max(X, axis =0) - np.min(X, axis =0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56ebf3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a normal gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_normal(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter:\n",
    "        theta_old = theta_new\n",
    "        theta_new = theta_old - learning_rate * gradient_eff(theta_old)\n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d37525c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.00282054, -0.66959885, -0.00840943,  0.47905071,\n",
       "         0.00443768]),\n",
       " 6441]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_normal(X_train, y_train, gradient_ols, learning_rate= 0.1, max_iter= 100000, precision=0.000001, stochastic=False, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c79ef59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a momentum gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "#momentum: float used for stochastic gradient descent\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_momentum(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None, momentum = 0.9):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    change = 0\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        theta_old = theta_new\n",
    "        change = learning_rate * gradient_eff(theta_old) + momentum * change\n",
    "        theta_new = theta_old - change \n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b4aba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a Adagrad gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "#epsilon: float used for gradient descent\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_adagrad(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None, epsilon = 1e-7):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    G = np.zeros(n_features)\n",
    "    gradient1 = np.zeros(n_features)\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter:\n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        G = G + np.square(gradient1)\n",
    "        theta_new = theta_old - learning_rate * gradient1 / np.sqrt(epsilon + G)\n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81bc2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a RMS Prop gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "#epsilon: float used for gradient descent\n",
    "#rho: float used for gradient descent\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_rmsprop(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None, epsilon = 1e-7 , rho= 0.9):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "        \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + np.ones(n_features)\n",
    "    v = np.zeros(n_features)\n",
    "    gradient1 = np.zeros(n_features)\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        v= rho * v + (1-rho) * gradient1**2\n",
    "        theta_new = theta_old - learning_rate / np.sqrt(v + epsilon) * gradient1\n",
    "        count += 1    \n",
    "    \n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "794b20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a Adam gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "#epsilon: float used for gradient descent\n",
    "#beta_1: float used for gradient descent\n",
    "#beta_2: float used for gradient descent\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_adam(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None,  epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + np.ones(n_features)\n",
    "    count = 0\n",
    "    m = 0\n",
    "    v = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        count+=1\n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        m = beta_1 * m + (1-beta_1)*gradient1\n",
    "        v = beta_2 * v + (1-beta_2)*gradient1**2\n",
    "        m_tilde = m/(1-beta_1**count)\n",
    "        v_tilde = v/(1-beta_2**count)\n",
    "        theta_new = theta_old - learning_rate * m_tilde / (np.sqrt(v_tilde) + epsilon)\n",
    "\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbc31f",
   "metadata": {},
   "source": [
    "Finding the optimal learning rates via Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "# tuning to find the best hyperparameters\n",
    "# parameters:\n",
    "# function: optimizer function which depends on one parameter and returns a list with the calculated optimium\n",
    "# and the number of iterations, particularly the gradient descent methods are such functions if all input parameters are fixed\n",
    "# apart from one hyperparameter\n",
    "# parameters: list of the parameters which are possible input values for function\n",
    "# objective: objective of the optimizer function, possible functions are\n",
    "# objective_ols or the returned values of maker_gradient_ridge or maker_gradient_lasso\n",
    "# returns:\n",
    "# a list containing a Pandas DataFrame with the columns Parameters, Result Objective and Number of iterations and dictionary with the optimum for each parameter\n",
    "def tuning(function, parameters, objective):\n",
    "    result_objective = np.zeros(len(parameters))\n",
    "    result_iterations = np.zeros(len(parameters))\n",
    "    result_optimum = {}\n",
    "    for i, parameter in enumerate(parameters):\n",
    "        print(f\"test parameter {parameter}\")\n",
    "        try:\n",
    "            function_run = function(parameter)\n",
    "            result_objective[i] = objective(function_run[0])\n",
    "            result_iterations[i] = function_run[1]\n",
    "            result_optimum[parameter] = function_run[0]\n",
    "        except RuntimeWarning:\n",
    "            result_objective[i] = np.inf\n",
    "            result_iterations[i] = np.inf\n",
    "            print(\"error\")\n",
    "\n",
    "    dataframe = pd.DataFrame({\"Parameters\": parameters, \"Result Objective\":result_objective, \"Number of Iterations\": result_iterations})\n",
    "    return [dataframe, result_optimum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ea0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          0.894485               10000.0\n",
      "1    0.000010          1.751385               10000.0\n",
      "2    0.000100          0.249079               10000.0\n",
      "3    0.001000          0.050915               10000.0\n",
      "4    0.010000          0.027417               10000.0\n",
      "5    0.100000          0.025538                7052.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent ols non stochastic\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, gradient_ols, learning_rate =param, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_normal_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3053db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00337317 -0.66959392 -0.00999685  0.47904601  0.00554336]\n"
     ]
    }
   ],
   "source": [
    "values_normal_ols_nonstochastic = results_normal_ols_nonstochastic[1][0.1]\n",
    "print(values_normal_ols_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bd2bb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "test parameter 0.1\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001         10.547166               10000.0\n",
      "1    0.000010          7.075605               10000.0\n",
      "2    0.000100          0.847404               10000.0\n",
      "3    0.001000          0.047890               10000.0\n",
      "4    0.010000          0.048526                7304.0\n",
      "5    0.100000          0.048526                 743.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent ridge non stochastic\n",
    "lmbda = 0.1\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, maker_gradient_ridge(lmbda), learning_rate =param, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_ridge_nonstochastic = tuning(function, parameters, maker_objective_ridge(X_train, y_train, lmbda = lmbda))\n",
    "print(results_normal_ridge_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d543daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          1.31206975 -0.88268098 -0.62116035  0.70449394 -0.66243896]\n"
     ]
    }
   ],
   "source": [
    "values_normal_ridge_nonstochastic = results_normal_ridge_nonstochastic[1][0.0001]\n",
    "print(values_normal_ridge_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21bf3fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.00302554, -0.6676647 , -0.00904645,  0.47710896,\n",
       "        0.00488546])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ridge(alpha =0.1).fit(X_train, y_train).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f598c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculation limit exceeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([ 0.        , -0.20772495, -0.79202489,  0.25354047,  0.60757   ,\n",
       "        -0.05980012]),\n",
       " 50000]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_normal(X_train, y_train, maker_gradient_ridge(lmbda), learning_rate =0.0001, max_iter = 50000, precision = 0.00000001, stochastic = False, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e68c711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          8.980139               50000.0\n",
      "1    0.000010          0.871942               50000.0\n",
      "2    0.000100          0.239399               50000.0\n",
      "3    0.001000          0.062085               50000.0\n",
      "4    0.010000          0.062230               50000.0\n",
      "5    0.100000          0.064887               50000.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent lasso non stochastic\n",
    "lmbda = 0.1\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, maker_gradient_lasso(lmbda), learning_rate =param, max_iter = 50000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_lasso_nonstochastic = tuning(function, parameters, maker_objective_lasso(X_train, y_train, lmbda = lmbda))\n",
    "print(results_normal_lasso_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "baf55989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00 -2.59732813e-05 -1.59179595e-01  3.94478382e-05\n",
      "  2.50245257e-05  4.81079302e-05]\n"
     ]
    }
   ],
   "source": [
    "values_normal_lasso_nonstochastic = results_normal_lasso_nonstochastic[1][0.001]\n",
    "print(values_normal_lasso_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7e896c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "calculation limit exceeded\n",
      "test parameter 10\n",
      "calculation limit exceeded\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          8.980139               50000.0\n",
      "1    0.000010          0.871942               50000.0\n",
      "2    0.000100          0.239399               50000.0\n",
      "3    0.001000          0.062085               50000.0\n",
      "4    0.010000          0.062230               50000.0\n",
      "5    0.100000          0.064887               50000.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for adam gradient descent lasso non stochastic\n",
    "lmbda = 0.1\n",
    "function = lambda param: gradient_descent_adam(X_train, y_train, maker_gradient_lasso(lmbda), learning_rate =param, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-7,max_iter = 50000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_adam_lasso_nonstochastic = tuning(function, parameters, maker_objective_lasso(X_train, y_train, lmbda = lmbda))\n",
    "print(results_normal_lasso_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a57900fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00 -2.59732813e-05 -1.59179595e-01  3.94478382e-05\n",
      "  2.50245257e-05  4.81079302e-05]\n"
     ]
    }
   ],
   "source": [
    "values_normal_lasso_nonstochastic = results_normal_lasso_nonstochastic[1][0.001]\n",
    "print(values_normal_lasso_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d20b8608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.       , -0.       , -0.1086299, -0.       , -0.       ,\n",
       "       -0.       ])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso(alpha = 0.1).fit(X_train, y_train).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3aac838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          2.400984                   1.0\n",
      "1    0.000010          5.419392               10000.0\n",
      "2    0.000100          5.930559               10000.0\n",
      "3    0.001000          0.072172               10000.0\n",
      "4    0.010000          0.048857               10000.0\n",
      "5    0.100000          0.025540               10000.0\n",
      "6    1.000000          0.025538                4469.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent ols stochastic\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, gradient_ols, learning_rate =param, max_iter = 10000, precision = 0.000001, stochastic = True, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_ols_stochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_normal_ols_stochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "605bb4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00329015 -0.66959466 -0.00975837  0.47904672  0.00537725]\n"
     ]
    }
   ],
   "source": [
    "values_normal_ols_stochastic = results_normal_ols_stochastic[1][1]\n",
    "print(values_normal_ols_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fadf31ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          7.084845               10000.0\n",
      "1    0.000010          6.774898               10000.0\n",
      "2    0.000100          0.092126               10000.0\n",
      "3    0.001000          0.055868               10000.0\n",
      "4    0.010000          0.025627               10000.0\n",
      "5    0.100000          0.025538                4368.0\n",
      "6    1.000000               inf                   0.0\n",
      "7   10.000000               inf                   0.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for momentum gradient descent ols\n",
    "function = lambda param: gradient_descent_momentum(X_train, y_train, gradient_ols, learning_rate =param, momentum = 0.3, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_momentum_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_momentum_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4986da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00290373 -0.66959811 -0.00864841  0.47905     0.00460413]\n"
     ]
    }
   ],
   "source": [
    "values_momentum_ols_nonstochastic = results_momentum_ols_nonstochastic[1][0.1]\n",
    "print(values_momentum_ols_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36043390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "test parameter 1e-05\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "test parameter 10\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001         24.641544                   5.0\n",
      "1    0.000010         11.940741                 500.0\n",
      "2    0.000100         21.821853               10000.0\n",
      "3    0.001000         11.633731               10000.0\n",
      "4    0.010000          0.164456               10000.0\n",
      "5    0.100000          0.025690               10000.0\n",
      "6    1.000000          0.025538                3378.0\n",
      "7   10.000000          0.025538                3711.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for adagrad gradient descent ols\n",
    "function = lambda param: gradient_descent_adagrad(X_train, y_train, gradient_ols, learning_rate =param, epsilon = 1e-7, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_adagrad_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_adagrad_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "863ed3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00294851 -0.66959765 -0.00877704  0.47904956  0.00469377]\n"
     ]
    }
   ],
   "source": [
    "values_adagrad_ols_nonstochastic = results_adagrad_ols_nonstochastic[1][10]\n",
    "print(values_adagrad_ols_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50562647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "calculation limit exceeded\n",
      "test parameter 1\n",
      "calculation limit exceeded\n",
      "test parameter 10\n",
      "calculation limit exceeded\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001         34.785867               10000.0\n",
      "1    0.000010         20.407346               10000.0\n",
      "2    0.000100          0.263431               10000.0\n",
      "3    0.001000          0.025542               10000.0\n",
      "4    0.010000          0.025864               10000.0\n",
      "5    0.100000          0.058106               10000.0\n",
      "6    1.000000          3.282334               10000.0\n",
      "7   10.000000        325.705101               10000.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for rmsprop gradient descent ols\n",
    "function = lambda param: gradient_descent_rmsprop(X_train, y_train, gradient_ols, learning_rate =param, epsilon = 1e-7, rho= 0.9, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_rmsprop_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_rmsprop_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4953b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00359395 -0.66910239 -0.00870584  0.47954231  0.00548765]\n"
     ]
    }
   ],
   "source": [
    "values_rmsprop_ols_nonstochastic = results_rmsprop_ols_nonstochastic[1][0.001]\n",
    "print(values_rmsprop_ols_nonstochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5196b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "test parameter 0.1\n",
      "test parameter 1\n",
      "test parameter 10\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001         10.629663               10000.0\n",
      "1    0.000010         15.830854               10000.0\n",
      "2    0.000100          0.807611               10000.0\n",
      "3    0.001000          0.028278               10000.0\n",
      "4    0.010000          0.025538                5728.0\n",
      "5    0.100000          0.025538                1998.0\n",
      "6    1.000000          0.025538                 276.0\n",
      "7   10.000000          0.025538                 557.0\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for adam gradient descent ols\n",
    "function = lambda param: gradient_descent_adam(X_train, y_train, gradient_ols, learning_rate =param, epsilon = 1e-7, beta_1= 0.9, beta_2= 0.999, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_adam_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_adam_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cff01b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00310434 -0.66959632 -0.00922465  0.4790483   0.00500549]\n"
     ]
    }
   ],
   "source": [
    "values_adam_ols_nonstochastic = results_adam_ols_nonstochastic[1][10]\n",
    "print(values_adam_ols_nonstochastic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oslo_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
