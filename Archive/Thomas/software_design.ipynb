{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,max_iter, precision, regression_method, gd_method, stochastic,batch_size, lmbda, hparameters_for_gd_method):\n",
    "#regression_method={ols, ridge, lasso}\n",
    "#gd_method ={normal, momentum, adagrad, rmsprop, adam}\n",
    "#stochastic: boolean whether to use stochastic gradient descent\n",
    "#batch_size: only used for stochstic gradient descent\n",
    "#lmbda: hyperparameter for ridge and lasso, for ols not needed\n",
    "#hparameters_for_gd_method: List, with e.g. initial point, learning rate and momentum for momentum\n",
    "    if regression_method = ols:\n",
    "        if stochastic:\n",
    "            def gradient(theta):\n",
    "                X_for_grad = ...#random\n",
    "                y_for_grad = ...#random\n",
    "                return X_for_grad.T @ X_for_grad @theta - X_for_grad.T @y_for_grad\n",
    "        else:\n",
    "            xtx = X_for_grad.T @ X_for_grad #precalculate stuff\n",
    "            xty = ...\n",
    "            def gradient(theta):\n",
    "                return xtx @ theta - xty\n",
    "\n",
    "    elif regression_method == lasso:\n",
    "        ....\n",
    "        def gradient(theta):\n",
    "            return ...\n",
    "\n",
    "    elif regression_method == ridge:\n",
    "        ...\n",
    "        def gradient (theta):\n",
    "            return ...\n",
    "    \n",
    "    else:\n",
    "        def gradient(theta): #python needs that\n",
    "            return ...\n",
    "        throw Error\n",
    "\n",
    "    \n",
    "    if gd_method == normal:\n",
    "        while stopping criterion:\n",
    "            theta = theta_old - hparameters_for_gd_method[0] * gradient(theta_old)\n",
    "        return theta\n",
    "\n",
    "    elif gd_method == momentum:\n",
    "        .....\n",
    "\n",
    "    ....\n",
    "\n",
    "#if you think one function gradient_descent for everything is too much, you could define one function for each gd_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b6dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if our task is not only to write functions for gradient descent which we can handle, but which are also userfriendly enough for a library, the following code could be nice\n",
    "\n",
    "def ols_gradient_descent(X,y,max_iter, precision, gd_method, stochastic, batch_size, hparameters_for_gd_method):\n",
    "    return gradient_descent(X,y,max_iter, precision, regression_method = ols, gd_method, stochastic, lmbda =0, hparameters_for_gd_method)\n",
    "\n",
    "def ridge_gradient_descent(...):\n",
    "    return ...\n",
    "\n",
    "def lasso_gradient_descent(....):\n",
    "    return ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oslo_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
