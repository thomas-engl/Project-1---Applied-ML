{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5750e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c53ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.40170629 -0.93565883  0.01553796 -0.74689327 -0.01629473]\n",
      " [-1.5447912   1.52856062 -1.87796819  1.59948217 -1.87381405]\n",
      " [ 0.94817042 -0.09404161  0.43015203 -0.40268479  0.15587129]\n",
      " ...\n",
      " [ 0.70088049 -0.55817176  0.16363592 -0.65115907  0.02037541]\n",
      " [ 0.15456285 -1.09551752 -0.01780354 -0.75867865 -0.01881364]\n",
      " [-0.56259062 -0.78097971 -0.10701462 -0.71920253 -0.030115  ]]\n"
     ]
    }
   ],
   "source": [
    "#Generate Data\n",
    "\n",
    "f = lambda x: 1/(1+25*x**2)\n",
    "np.random.seed(123)\n",
    "n_samples = 1000\n",
    "x_=np.random.uniform(-1,1,n_samples)\n",
    "noise = np.random.normal(loc=0, scale=0.1, size=n_samples)\n",
    "y_with_noise = f(x_) + noise\n",
    "degree = 5\n",
    "X = PolynomialFeatures(degree,include_bias=False).fit_transform(x_.reshape(-1,1))\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "y_centered = y_with_noise - y_with_noise.mean()\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X_scaled, y_centered, test_size = 0.3)\n",
    "y_train = y_train_ - y_train_.mean()\n",
    "y_test = y_test_ - y_train.mean()\n",
    "scaler = StandardScaler().fit(X_train_)\n",
    "X_train = scaler.transform(X_train_)\n",
    "X_test = scaler.transform(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0afb2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define makers for the objectives of OLS, Ridge and Lasso\n",
    "#It returns the corresponding objective: a one dimensional function\n",
    "#which only depends on theta and is minimized in regression\n",
    "def maker_objective_ols(X,y):\n",
    "    def objective_ols(theta):\n",
    "        return mean_squared_error(X @ theta, y)\n",
    "    return objective_ols\n",
    "\n",
    "def maker_objective_ridge(X, y, lmbda):\n",
    "    def objective_ridge(theta):\n",
    "        return mean_squared_error(X @ theta, y) + lmbda * np.sum(theta**2)\n",
    "    return objective_ridge\n",
    "\n",
    "def maker_objective_lasso(X, y, lmbda):\n",
    "    def objective_lasso(theta):\n",
    "        return mean_squared_error(X @ theta, y) + lmbda *np.sum(np.abs(theta))\n",
    "    return objective_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5c940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradients for OLS, Ridge and Lasso\n",
    "#For Ridge and Lasso a maker is used which takes lambda as input and returns\n",
    "#the corresponding gradient\n",
    "#definitions a\n",
    "#Parameters of the gradients:\n",
    "#xtx: np.array matrix product of X.T @ X\n",
    "#xty: np.array matrix product of X.T @ y\n",
    "#theta: np.array \n",
    "#n_samples: int number of samples i.e. the number of rows of X\n",
    "\n",
    "def gradient_ols(xtx, xty, theta, n_samples):\n",
    "    return 2/n_samples* (xtx @theta - xty)\n",
    "\n",
    "def maker_gradient_ridge(lmbda):\n",
    "    def gradient_ridge(xtx, xty, theta, n_samples):\n",
    "        return 2/n_samples*(xtx @theta - xty) + lmbda*theta\n",
    "    return gradient_ridge\n",
    "\n",
    "def maker_gradient_lasso(lmbda):\n",
    "    def gradient_lasso(xtx, xty, theta, n_samples):\n",
    "        return 2/n_samples*(xtx @theta - xty) + lmbda*np.sign(theta)\n",
    "    return gradient_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9720edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the effective gradient, i.e. for non stochastic gradient the normal\n",
    "#gradient with precalculated xtx and xty and for stochastic gradient random batches are created,\n",
    "#the gradient for every batch is calculated and effective gradient is the average gradient of all batches\n",
    "#is used in the gradient descent implementations\n",
    "#parameters:\n",
    "#X np.array\n",
    "#y np.array\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch_size: int used for gradient descent\n",
    "def gradient_eff_calculator(X, y, gradient, stochastic, batch_size):\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    if stochastic:\n",
    "        def gradient_eff(theta):\n",
    "            shuffled_indices = np.random.choice(range(n_samples), n_samples, replace = False)\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "            m = int(n_samples/batch_size) # number of batches\n",
    "            array_batch_gradients = np.zeros((m, n_features))\n",
    "            for i in range(m):\n",
    "                xi = X_shuffled[i*batch_size:(i+1)*batch_size]\n",
    "                yi = y_shuffled[i*batch_size: (i+1)*batch_size] #exclude the last samples?\n",
    "                batch_gradient_i = gradient(xi.T @ xi, xi.T @ yi, theta, n_samples)\n",
    "                array_batch_gradients[i] = batch_gradient_i\n",
    "            return np.mean(array_batch_gradients, axis = 0)\n",
    "    else:\n",
    "        xtx = X.T @ X\n",
    "        xty = X.T @ y\n",
    "        def gradient_eff(theta):\n",
    "            return gradient(xtx, xty, theta, n_samples)    \n",
    "    return gradient_eff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0663beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates a random initial value for the gradient decents methods\n",
    "#multivariate uniform distribution where the bounds for every feature\n",
    "#are the corresponding minima and maxima in X\n",
    "#used in the gradient descents methods\n",
    "def initial_value_generator(X):\n",
    "    n_features = X.shape[1]\n",
    "    return np.min(X, axis =0) + np.random.uniform(size =n_features) * (np.max(X, axis =0) - np.min(X, axis =0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ebf3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a normal gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_normal(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter:\n",
    "        theta_old = theta_new\n",
    "        theta_new = theta_old - learning_rate * gradient_eff(theta_old)\n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d37525c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.00282054, -0.66959885, -0.00840943,  0.47905071,\n",
       "         0.00443768]),\n",
       " 6441]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent_normal(X_train, y_train, gradient_ols, learning_rate= 0.1, max_iter= 100000, precision=0.000001, stochastic=False, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79ef59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a momentum gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "#momentum: float used for stochastic gradient descent\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_momentum(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None, momentum = 0.9):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    change = 0\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        theta_old = theta_new\n",
    "        change = learning_rate * gradient_eff(theta_old) + momentum * change\n",
    "        theta_new = theta_old - change \n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b4aba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a Adagrad gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "#epsilon: float used for gradient descent\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_adagrad(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None, epsilon = 1e-7):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + 1\n",
    "    G = np.zeros(n_features)\n",
    "    gradient1 = np.zeros(n_features)\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter:\n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        G = G + np.square(gradient1)\n",
    "        theta_new = theta_old - learning_rate * gradient1 / np.sqrt(epsilon + G)\n",
    "        count += 1\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81bc2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a RMS Prop gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "#epsilon: float used for gradient descent\n",
    "#rho: float used for gradient descent\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_rmsprop(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None, epsilon = 1e-7 , rho= 0.9):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "        \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + np.ones(n_features)\n",
    "    v = np.zeros(n_features)\n",
    "    gradient1 = np.zeros(n_features)\n",
    "    count = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        v= rho * v + (1-rho) * gradient1**2\n",
    "        theta_new = theta_old - learning_rate / np.sqrt(v + epsilon) * gradient1\n",
    "        count += 1    \n",
    "    \n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "794b20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates a Adam gradient descent\n",
    "\n",
    "#parameters:\n",
    "#X: np.array features matrix\n",
    "#y: np.arry targets\n",
    "#gradient: function with parameters xtx, xty, theta and number_of samples which returns the gradient at theta as np.array\n",
    "#possible functions are gradient_ols, or the returned functions of maker_gradient_ridge or maker_gradient_lasso\n",
    "#learning rate: float used for stochastic gradient descent\n",
    "#max_iter: int maximum iteration number in gradient descent\n",
    "#precision: float stopping criterion before iteration limit is defined as ||theta_old - theta_new||_2 <= precision\n",
    "#stochastic: boolean if stochastic gradient descent should be used\n",
    "#batch size: int used for stochastic gradient descent\n",
    "#initial value: np.array for gradient descent, if None a random value in range of X is chosen\n",
    "#epsilon: float used for gradient descent\n",
    "#beta_1: float used for gradient descent\n",
    "#beta_2: float used for gradient descent\n",
    "\n",
    "#returns a list containing the calculated theta and the number of used iterations\n",
    "def gradient_descent_adam(X, y, gradient, learning_rate, max_iter, precision, stochastic, batch_size, initial_value = None,  epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "    X = X.astype(np.float64)\n",
    "    y = y.astype(np.float64)\n",
    "    n_features = X.shape[1]\n",
    "    gradient_eff = gradient_eff_calculator(X, y, gradient, stochastic, batch_size)\n",
    "\n",
    "    if initial_value is None:\n",
    "        initial_value = initial_value_generator(X)\n",
    "    \n",
    "    theta_new = initial_value \n",
    "    theta_old = initial_value + np.ones(n_features)\n",
    "    count = 0\n",
    "    m = 0\n",
    "    v = 0\n",
    "    while (np.linalg.norm(theta_old - theta_new, ord = None) > precision) and count < max_iter: \n",
    "        count+=1\n",
    "        theta_old = theta_new\n",
    "        gradient1 = gradient_eff(theta_old)\n",
    "        m = beta_1 * m + (1-beta_1)*gradient1\n",
    "        v = beta_2 * v + (1-beta_2)*gradient1**2\n",
    "        m_tilde = m/(1-beta_1**count)\n",
    "        v_tilde = v/(1-beta_2**count)\n",
    "        theta_new = theta_old - learning_rate * m_tilde / (np.sqrt(v_tilde) + epsilon)\n",
    "\n",
    "    if(count == max_iter):\n",
    "        print('calculation limit exceeded')\n",
    "    return [theta_new, count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbc31f",
   "metadata": {},
   "source": [
    "Finding the optimal learning rates via Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f19351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "# tuning to find the best hyperparameters\n",
    "# parameters:\n",
    "# function: optimizer function which depends on one parameter and returns a list with the calculated optimium\n",
    "# and the number of iterations, particularly the gradient descent methods are such functions if all input parameters are fixed\n",
    "# apart from one hyperparameter\n",
    "# parameters: list of the parameters which are possible input values for function\n",
    "# objective: objective of the optimizer function, possible functions are\n",
    "# objective_ols or the returned values of maker_gradient_ridge or maker_gradient_lasso\n",
    "# returns:\n",
    "# a list containing a Pandas DataFrame with the columns Parameters, Result Objective and Number of iterations and dictionary with the optimum for each parameter\n",
    "def tuning(function, parameters, objective):\n",
    "    result_objective = np.zeros(len(parameters))\n",
    "    result_iterations = np.zeros(len(parameters))\n",
    "    result_optimum = {}\n",
    "    for i, parameter in enumerate(parameters):\n",
    "        print(f\"test parameter {parameter}\")\n",
    "        try:\n",
    "            function_run = function(parameter)\n",
    "            result_objective[i] = objective(function_run[0])\n",
    "            result_iterations[i] = function_run[1]\n",
    "            result_optimum[parameter] = function_run[0]\n",
    "        except RuntimeWarning:\n",
    "            result_objective[i] = np.inf\n",
    "            result_iterations[i] = np.inf\n",
    "            print(\"error\")\n",
    "\n",
    "    dataframe = pd.DataFrame({\"Parameters\": parameters, \"Result Objective\":result_objective, \"Number of Iterations\": result_iterations})\n",
    "    return [dataframe, result_optimum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ea0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test parameter 1e-06\n",
      "calculation limit exceeded\n",
      "test parameter 1e-05\n",
      "calculation limit exceeded\n",
      "test parameter 0.0001\n",
      "calculation limit exceeded\n",
      "test parameter 0.001\n",
      "calculation limit exceeded\n",
      "test parameter 0.01\n",
      "calculation limit exceeded\n",
      "test parameter 0.1\n",
      "test parameter 1\n",
      "error\n",
      "test parameter 10\n",
      "error\n",
      "   Parameters  Result Objective  Number of Iterations\n",
      "0    0.000001          0.894485               10000.0\n",
      "1    0.000010          1.751385               10000.0\n",
      "2    0.000100          0.249079               10000.0\n",
      "3    0.001000          0.050915               10000.0\n",
      "4    0.010000          0.027417               10000.0\n",
      "5    0.100000          0.025538                7052.0\n",
      "6    1.000000               inf                   inf\n",
      "7   10.000000               inf                   inf\n"
     ]
    }
   ],
   "source": [
    "#find learning rate for normal gradient descent ols non stochastic\n",
    "function = lambda param: gradient_descent_normal(X_train, y_train, gradient_ols, learning_rate =param, max_iter = 10000, precision = 0.000001, stochastic = False, batch_size=100)\n",
    "parameters =[10**i for i in range(-6, 2)]\n",
    "results_normal_ols_nonstochastic = tuning(function, parameters, maker_objective_ols(X_train, y_train))\n",
    "print(results_normal_ols_nonstochastic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3053db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.00337317 -0.66959392 -0.00999685  0.47904601  0.00554336]\n"
     ]
    }
   ],
   "source": [
    "values_normal_ols_nonstochastic = results_normal_ols_nonstochastic[1][0.1]\n",
    "print(values_normal_ols_nonstochastic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oslo_ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
